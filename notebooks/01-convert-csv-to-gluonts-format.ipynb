{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div style=\"font-size:200%\">Create gluonts DataSet</div>**\n",
    "\n",
    "This notebook demonstrates how to\n",
    "\n",
    "1. create a gluonts dataset which consists of train split, test split, and metadata.\n",
    "2. add label encodings as a custom metadata to the gluonts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Sequence\n",
    "\n",
    "import pandas as pd\n",
    "from gluonts.dataset.common import (\n",
    "    CategoricalFeatureInfo,\n",
    "    ListDataset,\n",
    "    MetaData,\n",
    "    TrainDatasets,\n",
    "    load_datasets,\n",
    ")\n",
    "\n",
    "from gluonts_nb_utils import fill_dt_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the .csv file produced by notebook 00-setup-env.ipynb.\n",
    "csv_fname = '../data/input_to_forecast.csv'\n",
    "\n",
    "# Constants used by this example.\n",
    "freq, fcast_length = \"D\", 30\n",
    "min_date = '2014-01-01'\n",
    "max_date = '2014-12-31'\n",
    "\n",
    "# Output\n",
    "bucket = 'OUTPUT_BUCKET'\n",
    "prefix = 'gluonts-examples-dataset'\n",
    "dataset_name = f'synthetic-dataset'\n",
    "\n",
    "%set_env BUCKET=$bucket\n",
    "%set_env PREFIX=$prefix\n",
    "%set_env DATASET_NAME=$dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Filter Timeseries\n",
    "\n",
    "In this step, we load the synthetic data generated in notebook `00-setup-env.ipynb` with the following steps:\n",
    "\n",
    "1. Read .csv file.\n",
    "2. Select specific period.\n",
    "3. Apply mid-padding and right-padding (with zero) to ensure gluonts see contiguous timeseries with the same end timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load .csv whose timeseries can be fragmented.\n",
    "ts = pd.read_csv('../data/input_to_forecast.csv', parse_dates=['timestamp'], infer_datetime_format=True)\n",
    "ts.rename(columns={'timestamp': 'x', 'quantity': 'y'}, inplace=True)\n",
    "\n",
    "# Select only raw data within a specific year.\n",
    "ts = ts[(ts['x'] >= min_date) & (ts['x'] <= max_date)]\n",
    "\n",
    "# For each SKU, perform mid-padding and right-padding\n",
    "ts_filled = fill_dt_all(ts, ts_id=['sku'], dates=(\"min\", max_date, \"D\"), freq=freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate gluonts TRAIN dataset\n",
    "\n",
    "Implementation notes:\n",
    "\n",
    "- For simplicity, this implementation keeps all the generated gluonts timeseries in memory.\n",
    "\n",
    "- We use `gluonts.dataset.common.TrainDatasets.save()` which writes to local filesystem, hence needs a follow-up step to upload to s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN: in-memory gluonts data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(cats):\n",
    "    return {c: i for i, c in enumerate(cats)}\n",
    "\n",
    "\n",
    "def df2gluonts(\n",
    "    df,\n",
    "    cat_idx,\n",
    "    fcast_len: int,\n",
    "    freq: str = \"D\",\n",
    "    ts_id: Sequence[str] = [\"cat\", \"cc\"],\n",
    "    static_cat: Sequence[str] = [\"cat\", \"cc\"],\n",
    "    item_id_fn: Callable = None,\n",
    ") -> None:\n",
    "    \"\"\"Convert a dataframe of multiple timeseries to json lines.\n",
    "\n",
    "    This function supports gluonts static features, but not the dynamic features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Dataframe of multiple timeseries, where target variable must be called column `y`.\n",
    "        cat_idx (Dict[str, Dict[str, int]]): Mapper for static categories.\n",
    "        fcast_len (int, optional): Forecast horizon. Defaults to 12.\n",
    "        freq (str, optional): Frequency of timeseries. Defaults to 'W'.\n",
    "        ts_id (Sequence[str], optional): Identifier columns in the dataframe. Defaults to ['cat', 'cc'].\n",
    "        static_cat (Sequence[str], optional): Columns that denotes static category features of each timeseries.\n",
    "            Defaults to ['cat', 'cc'].\n",
    "        item_id_fn ([type], optional): Function to format `item_id`. Defaults to None.\n",
    "    \"\"\"\n",
    "    data_iter = []\n",
    "\n",
    "    # Build the payload\n",
    "    for item_id, dfg in df.groupby(ts_id, as_index=False):\n",
    "        if len(ts_id) < 2:\n",
    "            item_id = [item_id]\n",
    "\n",
    "        if fcast_len > 0:\n",
    "            # Train split exclude the last fcast_len timestamps\n",
    "            ts_len = len(dfg) - fcast_len\n",
    "            target = dfg[\"y\"][:-fcast_len]\n",
    "        else:\n",
    "            # Test split include all timeseries. During backtesting,\n",
    "            # gluonts will treat the fcast_len as groundtruth.\n",
    "            target = dfg[\"y\"]\n",
    "\n",
    "        feat_static_cat = []\n",
    "        for col in static_cat:\n",
    "            # Construct all static category features of current timeseries.\n",
    "            assert dfg[col].nunique() == 1\n",
    "            cat_value = dfg[col].iloc[0]\n",
    "            # Encode sku to zero-based number for feat_static_cat.\n",
    "            feat_static_cat.append(cat_idx[col][cat_value])\n",
    "\n",
    "        if item_id_fn is None:\n",
    "            # NOTE: our sm-glounts entrypoint will interpret '|' as '\\n'\n",
    "            # in the plot title.\n",
    "            item_id = \"|\".join(item_id)\n",
    "        else:\n",
    "            item_id = item_id_fn(*item_id)\n",
    "\n",
    "        data_iter.append(\n",
    "            {\"start\": dfg.iloc[0][\"x\"], \"target\": target, \"feat_static_cat\": feat_static_cat, \"item_id\": item_id}\n",
    "        )\n",
    "\n",
    "    # Finally we call gluonts API to convert data_iter with frequency of\n",
    "    # the observation in the time series\n",
    "    data = ListDataset(data_iter, freq=freq)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate train and test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-based SKU encoding.\n",
    "# TS is a pd.DataFrame that contains time series for each SKU,\n",
    "# where all timeseries have the same frequency.\n",
    "cat_inverted_idx = {'sku': encode_cat(ts_filled['sku'].unique())}\n",
    "\n",
    "# Drop the final fcast_length from train data.\n",
    "train_data= df2gluonts(ts_filled,\n",
    "                       cat_inverted_idx,\n",
    "                       fcast_len=fcast_length,\n",
    "                       freq=freq,\n",
    "                       ts_id=['sku'],\n",
    "                       static_cat=['sku']\n",
    ")\n",
    "\n",
    "# Test data include fcast_length which are ground truths.\n",
    "test_data = df2gluonts(ts_filled,\n",
    "                       cat_inverted_idx,\n",
    "                       fcast_len=0,\n",
    "                       freq=freq,\n",
    "                       ts_id=['sku'],\n",
    "                       static_cat=['sku']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN: write to local fs, then s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gluonts_datasets = TrainDatasets(\n",
    "    metadata=MetaData(\n",
    "                freq=freq,\n",
    "                target={'name': 'quantity'},\n",
    "                feat_static_cat=[\n",
    "                    CategoricalFeatureInfo(name=k, cardinality=len(v)+1)   # Add 'unknown'.\n",
    "                    for k,v in cat_inverted_idx.items()\n",
    "                ],\n",
    "                prediction_length = fcast_length\n",
    "    ),\n",
    "    train=train_data,\n",
    "    test=test_data\n",
    ")\n",
    "\n",
    "# Setting `overwrite=True` means rm -fr path_str, mkdir path_str, then write individual files.\n",
    "local_path=f'../data/processed/{dataset_name}'\n",
    "gluonts_datasets.save(path_str=local_path, overwrite=True)\n",
    "\n",
    "# Save also our indexes\n",
    "with open(Path(local_path) / 'metadata' / 'cat.json', 'w') as f:\n",
    "    json.dump(cat_inverted_idx, f)\n",
    "\n",
    "# Preview the generated json files.\n",
    "# NOTE: you can safely ignore 'Broken pipe' errors in the cell's output.\n",
    "%set_env LOCAL_PATH=$local_path\n",
    "!cat $LOCAL_PATH/metadata/metadata.json | head -1 | jq\n",
    "!cat $LOCAL_PATH/train/data.json | head -1 | jq\n",
    "!cat $LOCAL_PATH/test/data.json | head -1 | jq\n",
    "!cat $LOCAL_PATH/metadata/cat.json | jq '.' | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can re-read the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "reloaded_dataset = load_datasets(\n",
    "                        metadata=os.path.join(local_path, \"metadata\"),\n",
    "                        train=os.path.join(local_path, \"train\"),\n",
    "                        test=os.path.join(local_path, \"test\")\n",
    "                   )\n",
    "display(\n",
    "    reloaded_dataset.metadata,\n",
    "    reloaded_dataset.train,\n",
    "    reloaded_dataset.test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks ok. Now, let's upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $LOCAL_PATH s3://$BUCKET/$PREFIX/$DATASET_NAME/ --storage-class ONEZONE_IA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the uploaded files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls s3://$BUCKET/$PREFIX/$DATASET_NAME/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
