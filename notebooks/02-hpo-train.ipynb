{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div style='font-size:200%'>Minimalistic example for gluonts entrypoint script</div>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from time import gmtime, strftime \n",
    "\n",
    "import boto3\n",
    "import sagemaker as sm\n",
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner, IntegerParameter\n",
    "\n",
    "# A few standard SageMaker's stanzas\n",
    "role: str = get_sm_execution_role()\n",
    "sess = sm.Session()\n",
    "region: str = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s3_dataset': 's3://vm-hello-world/gluonts'}\n",
      "env: BUCKET=vm-hello-world\n",
      "env: PREFIX=gluonts\n"
     ]
    }
   ],
   "source": [
    "# Input data for training. Ensure the same as bucket, prefix, and dataset_name from 01-create-dataset.ipynb.\n",
    "bucket = 'BUCKET'\n",
    "prefix = 'PREFIX/DATASET_NAME'   # Ensure no trailing '/'.\n",
    "data_channels = {'s3_dataset': f's3://{bucket}/{prefix}'}\n",
    "print(data_channels)\n",
    "\n",
    "# Model hyperparameters. Here we demonstrate that:\n",
    "# - the dataset embeds its recommended forecast length (=12) in its metadata.\n",
    "# - However, each train job is free to override the forecast length. We override to 6 weeks.\n",
    "fcast_length = 6\n",
    "\n",
    "source_dir = '../src/entrypoint/'\n",
    "\n",
    "%set_env BUCKET=$bucket\n",
    "%set_env PREFIX=$prefix\n",
    "\n",
    "# Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "# we choose to capture only a few.\n",
    "metric=[\n",
    "    {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "    {\"Name\": \"test:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'validation_epoch_loss'=(\\S+)\"},\n",
    "    {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "    {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read cardinality of static feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF this cell fails, upgrade s3fs.\n",
    "with Path2(f's3://{bucket}/{prefix}/metadata/metadata.json').open('rb') as f:\n",
    "    cardinality = [int(json.load(f)['feat_static_cat'][0]['cardinality'])]\n",
    "cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave as an exercise to read the recommended forecast length in the dataset's metadata in `f's3://{bucket}/{prefix}/metadata/metadata.json'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same entrypoint script supports the various estimators in gluonts. You specify the estimator to the entrypoint script using hyperparameter `algo`. The next cell demonstrate the DeepAR estimator `gluonts.model.deepar.DeepAREstimator`.\n",
    "\n",
    "Feel free to experiment with another estimator e.g., `gluonts.model.deepstate.DeepStateEstimator`.\n",
    "\n",
    "Do note that you must consult the estimator's documentation for their hyperparameters aka. `kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-04-23 02:45:50 Starting - Starting the training job...\n",
      "2020-04-23 02:45:51 Starting - Launching requested ML instances......\n",
      "2020-04-23 02:47:19 Starting - Preparing the instances for training......\n",
      "2020-04-23 02:48:10 Downloading - Downloading input data...\n",
      "2020-04-23 02:48:51 Training - Training image download completed. Training in progress..\u001b[34m2020-04-23 02:48:52,171 sagemaker-containers INFO     Imported framework sagemaker_mxnet_container.training\u001b[0m\n",
      "\u001b[34m2020-04-23 02:48:52,174 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-04-23 02:48:52,186 sagemaker_mxnet_container.training INFO     MXNet training environment: {'SM_HOSTS': '[\"algo-1\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"algo\":\"gluonts.model.deepar.DeepAREstimator\",\"cardinality\":\"[5]\",\"distr_output\":\"gluonts.distribution.gaussian.GaussianOutput\",\"num_samples\":1000,\"plot_transparent\":0,\"prediction_length\":2,\"trainer\":\"gluonts.trainer.Trainer\",\"trainer.epochs\":2,\"use_feat_static_cat\":\"True\"}', 'SM_USER_ENTRY_POINT': 'entrypoint.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{\"s3_dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[\"s3_dataset\"]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'entrypoint', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_mxnet_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '2', 'SM_NUM_GPUS': '0', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': 's3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/source/sourcedir.tar.gz', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"s3_dataset\":\"/opt/ml/input/data/s3_dataset\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"algo\":\"gluonts.model.deepar.DeepAREstimator\",\"cardinality\":\"[5]\",\"distr_output\":\"gluonts.distribution.gaussian.GaussianOutput\",\"num_samples\":1000,\"plot_transparent\":0,\"prediction_length\":2,\"trainer\":\"gluonts.trainer.Trainer\",\"trainer.epochs\":2,\"use_feat_static_cat\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"s3_dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-04-23-02-46-03-719\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/source/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entrypoint.py\"}', 'SM_USER_ARGS': '[\"--algo\",\"gluonts.model.deepar.DeepAREstimator\",\"--cardinality\",\"[5]\",\"--distr_output\",\"gluonts.distribution.gaussian.GaussianOutput\",\"--num_samples\",\"1000\",\"--plot_transparent\",\"0\",\"--prediction_length\",\"2\",\"--trainer\",\"gluonts.trainer.Trainer\",\"--trainer.epochs\",\"2\",\"--use_feat_static_cat\",\"True\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_CHANNEL_S3_DATASET': '/opt/ml/input/data/s3_dataset', 'SM_HP_PREDICTION_LENGTH': '2', 'SM_HP_PLOT_TRANSPARENT': '0', 'SM_HP_DISTR_OUTPUT': 'gluonts.distribution.gaussian.GaussianOutput', 'SM_HP_CARDINALITY': '[5]', 'SM_HP_USE_FEAT_STATIC_CAT': 'True', 'SM_HP_TRAINER': 'gluonts.trainer.Trainer', 'SM_HP_TRAINER.EPOCHS': '2', 'SM_HP_ALGO': 'gluonts.model.deepar.DeepAREstimator', 'SM_HP_NUM_SAMPLES': '1000'}\u001b[0m\n",
      "\u001b[34m2020-04-23 02:48:52,407 sagemaker-containers INFO     Module entrypoint does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-04-23 02:48:52,407 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-04-23 02:48:52,407 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-04-23 02:48:52,407 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m pip install -U . -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\u001b[0m\n",
      "\u001b[34mCollecting gluonts==0.4.2 (from -r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/98/c8/113009b077ca127308470dcd4851e53a6b4ad905fe61f36f28d22ff3a4a5/gluonts-0.4.2-py3-none-any.whl (323kB)\u001b[0m\n",
      "\u001b[34mCollecting pydantic<1.5.* (from -r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/be/9a/a2d9613a70051615a84df6e9d697aad9787ba978bdeb4ad46c754457b3e1/pydantic-1.4-cp36-cp36m-manylinux2010_x86_64.whl (7.5MB)\u001b[0m\n",
      "\u001b[34mCollecting matplotlib~=3.0 (from gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/93/4b/52da6b1523d5139d04e02d9e26ceda6146b48f2a4e5d2abfdf1c7bac8c40/matplotlib-3.2.1-cp36-cp36m-manylinux1_x86_64.whl (12.4MB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: numpy~=1.14 in /usr/local/lib/python3.6/site-packages (from gluonts==0.4.2->-r requirements.txt (line 1)) (1.14.5)\u001b[0m\n",
      "\u001b[34mCollecting pandas<0.26,>=0.25 (from gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\u001b[0m\n",
      "\u001b[34mCollecting holidays<0.10,>=0.9 (from gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/51/2c/5289263b6bb3a1ac51ddfd1f631947e2636ad9ebe8ac5e88ec37bceffc11/holidays-0.9.12.tar.gz (85kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: boto3~=1.0 in /usr/local/lib/python3.6/site-packages (from gluonts==0.4.2->-r requirements.txt (line 1)) (1.9.176)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: python-dateutil==2.8.0 in /usr/local/lib/python3.6/site-packages (from gluonts==0.4.2->-r requirements.txt (line 1)) (2.8.0)\u001b[0m\n",
      "\u001b[34mCollecting tqdm~=4.23 (from gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/1c/6359be64e8301b84160f6f6f7936bbfaaa5e9a4eab6cbc681db07600b949/tqdm-4.45.0-py2.py3-none-any.whl (60kB)\u001b[0m\n",
      "\u001b[34mCollecting ujson~=1.35 (from gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\u001b[0m\n",
      "\u001b[34mCollecting dataclasses>=0.6; python_version < \"3.7\" (from pydantic<1.5.*->-r requirements.txt (line 2))\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/d2/6f02df2616fd4016075f60157c7a0452b38d8f7938ae94343911e0fb0b09/dataclasses-0.7-py3-none-any.whl\u001b[0m\n",
      "\u001b[34mCollecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 (from matplotlib~=3.0->gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/bb/488841f56197b13700afd5658fc279a2025a39e22449b7cf29864669b15d/pyparsing-2.4.7-py2.py3-none-any.whl (67kB)\u001b[0m\n",
      "\u001b[34mCollecting kiwisolver>=1.0.1 (from matplotlib~=3.0->gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/ae/23/147de658aabbf968324551ea22c0c13a00284c4ef49a77002e91f79657b7/kiwisolver-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (88kB)\u001b[0m\n",
      "\u001b[34mCollecting cycler>=0.10 (from matplotlib~=3.0->gluonts==0.4.2->-r requirements.txt (line 1))\n",
      "  Downloading https://files.pythonhosted.org/packages/f7/d2/e07d3ebb2bd7af696440ce7e754c59dd546ffe1bbe732c8ab68b9c834e61/cycler-0.10.0-py2.py3-none-any.whl\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/site-packages (from pandas<0.26,>=0.25->gluonts==0.4.2->-r requirements.txt (line 1)) (2019.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/site-packages (from holidays<0.10,>=0.9->gluonts==0.4.2->-r requirements.txt (line 1)) (1.12.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/site-packages (from boto3~=1.0->gluonts==0.4.2->-r requirements.txt (line 1)) (0.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.176 in /usr/local/lib/python3.6/site-packages (from boto3~=1.0->gluonts==0.4.2->-r requirements.txt (line 1)) (1.12.176)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/site-packages (from boto3~=1.0->gluonts==0.4.2->-r requirements.txt (line 1)) (0.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /usr/local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.176->boto3~=1.0->gluonts==0.4.2->-r requirements.txt (line 1)) (1.24.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.176->boto3~=1.0->gluonts==0.4.2->-r requirements.txt (line 1)) (0.14)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyparsing, kiwisolver, cycler, matplotlib, pandas, holidays, dataclasses, pydantic, tqdm, ujson, gluonts, entrypoint\u001b[0m\n",
      "\u001b[34m  Found existing installation: pandas 0.24.1\n",
      "    Uninstalling pandas-0.24.1:\n",
      "      Successfully uninstalled pandas-0.24.1\u001b[0m\n",
      "\u001b[34m  Running setup.py install for holidays: started\u001b[0m\n",
      "\u001b[34m    Running setup.py install for holidays: finished with status 'done'\n",
      "  Running setup.py install for ujson: started\u001b[0m\n",
      "\u001b[34m    Running setup.py install for ujson: finished with status 'done'\n",
      "  Running setup.py install for entrypoint: started\u001b[0m\n",
      "\u001b[34m    Running setup.py install for entrypoint: finished with status 'done'\u001b[0m\n",
      "\u001b[34mSuccessfully installed cycler-0.10.0 dataclasses-0.7 entrypoint-1.0.0 gluonts-0.4.2 holidays-0.9.12 kiwisolver-1.2.0 matplotlib-3.2.1 pandas-0.25.3 pydantic-1.4 pyparsing-2.4.7 tqdm-4.45.0 ujson-1.35\u001b[0m\n",
      "\u001b[34mWARNING: You are using pip version 19.1.1, however version 20.0.2 is available.\u001b[0m\n",
      "\u001b[34mYou should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[34m2020-04-23 02:49:02,773 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-04-23 02:49:02,787 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"s3_dataset\": \"/opt/ml/input/data/s3_dataset\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_mxnet_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"prediction_length\": 2,\n",
      "        \"plot_transparent\": 0,\n",
      "        \"distr_output\": \"gluonts.distribution.gaussian.GaussianOutput\",\n",
      "        \"cardinality\": \"[5]\",\n",
      "        \"use_feat_static_cat\": \"True\",\n",
      "        \"trainer\": \"gluonts.trainer.Trainer\",\n",
      "        \"trainer.epochs\": 2,\n",
      "        \"algo\": \"gluonts.model.deepar.DeepAREstimator\",\n",
      "        \"num_samples\": 1000\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"s3_dataset\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"mxnet-training-2020-04-23-02-46-03-719\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entrypoint\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entrypoint.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"algo\":\"gluonts.model.deepar.DeepAREstimator\",\"cardinality\":\"[5]\",\"distr_output\":\"gluonts.distribution.gaussian.GaussianOutput\",\"num_samples\":1000,\"plot_transparent\":0,\"prediction_length\":2,\"trainer\":\"gluonts.trainer.Trainer\",\"trainer.epochs\":2,\"use_feat_static_cat\":\"True\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entrypoint.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"s3_dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"s3_dataset\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entrypoint\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_mxnet_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"s3_dataset\":\"/opt/ml/input/data/s3_dataset\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_mxnet_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"algo\":\"gluonts.model.deepar.DeepAREstimator\",\"cardinality\":\"[5]\",\"distr_output\":\"gluonts.distribution.gaussian.GaussianOutput\",\"num_samples\":1000,\"plot_transparent\":0,\"prediction_length\":2,\"trainer\":\"gluonts.trainer.Trainer\",\"trainer.epochs\":2,\"use_feat_static_cat\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"s3_dataset\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"mxnet-training-2020-04-23-02-46-03-719\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/source/sourcedir.tar.gz\",\"module_name\":\"entrypoint\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entrypoint.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--algo\",\"gluonts.model.deepar.DeepAREstimator\",\"--cardinality\",\"[5]\",\"--distr_output\",\"gluonts.distribution.gaussian.GaussianOutput\",\"--num_samples\",\"1000\",\"--plot_transparent\",\"0\",\"--prediction_length\",\"2\",\"--trainer\",\"gluonts.trainer.Trainer\",\"--trainer.epochs\",\"2\",\"--use_feat_static_cat\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_S3_DATASET=/opt/ml/input/data/s3_dataset\u001b[0m\n",
      "\u001b[34mSM_HP_PREDICTION_LENGTH=2\u001b[0m\n",
      "\u001b[34mSM_HP_PLOT_TRANSPARENT=0\u001b[0m\n",
      "\u001b[34mSM_HP_DISTR_OUTPUT=gluonts.distribution.gaussian.GaussianOutput\u001b[0m\n",
      "\u001b[34mSM_HP_CARDINALITY=[5]\u001b[0m\n",
      "\u001b[34mSM_HP_USE_FEAT_STATIC_CAT=True\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINER=gluonts.trainer.Trainer\u001b[0m\n",
      "\u001b[34mSM_HP_TRAINER.EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_ALGO=gluonts.model.deepar.DeepAREstimator\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_SAMPLES=1000\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/usr/local/bin:/usr/local/lib/python36.zip:/usr/local/lib/python3.6:/usr/local/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.6 -m entrypoint --algo gluonts.model.deepar.DeepAREstimator --cardinality [5] --distr_output gluonts.distribution.gaussian.GaussianOutput --num_samples 1000 --plot_transparent 0 --prediction_length 2 --trainer gluonts.trainer.Trainer --trainer.epochs 2 --use_feat_static_cat True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mEnv. var SM_HOSTS detected. Silencing tqdm as we're likely to run on SageMaker...\u001b[0m\n",
      "\u001b[34mlevel: 0, name: __main__, handlers: []\u001b[0m\n",
      "\u001b[34mlevel: 20, name: root, handlers: [<StreamHandler <stderr> (NOTSET)>]\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] __main__ CLI args to entrypoint script: ['/opt/ml/code/entrypoint.py', '--algo', 'gluonts.model.deepar.DeepAREstimator', '--cardinality', '[5]', '--distr_output', 'gluonts.distribution.gaussian.GaussianOutput', '--num_samples', '1000', '--plot_transparent', '0', '--prediction_length', '2', '--trainer', 'gluonts.trainer.Trainer', '--trainer.epochs', '2', '--use_feat_static_cat', 'True']\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] __main__ Loading dataset from /opt/ml/input/data/s3_dataset\u001b[0m\n",
      "\u001b[34m/opt/ml/code/sm_util.py:95: RuntimeWarning: This implementation still ignores cardinality and static features in the metadata\n",
      "  warnings.warn(\"This implementation still ignores cardinality and static features in the metadata\", RuntimeWarning)\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] root Using CPU\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] root Using CPU\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] __main__ Estimator: gluonts.model.deepar._estimator.DeepAREstimator(cardinality=[5], cell_type=\"lstm\", context_length=None, distr_output=gluonts.distribution.gaussian.GaussianOutput(), dropout_rate=0.1, embedding_dimension=None, freq=\"D\", lags_seq=None, num_cells=40, num_layers=2, num_parallel_samples=100, prediction_length=2, scaling=True, time_features=None, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=2, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=50, patience=10, weight_decay=1e-08), use_feat_dynamic_real=False, use_feat_static_cat=True, use_feat_static_real=False)\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] __main__ Starting model training.\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] root Start model training\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:05] [INFO] root Epoch[0] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0% 0/50 [00:00<?, ?it/s][2020-04-23 02:49:05] [INFO] root Number of parameters in DeepARTrainingNetwork: 26177\u001b[0m\n",
      "\u001b[34m#015100% 50/50 [00:01<00:00, 49.67it/s, avg_epoch_loss=7.21]\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:06] [INFO] root Epoch[0] Elapsed time 1.009 seconds\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:06] [INFO] root Epoch[0] Evaluation metric 'epoch_loss'=7.214419\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:06] [INFO] root Epoch[1] Learning rate is 0.001\u001b[0m\n",
      "\u001b[34m#015  0% 0/50 [00:00<?, ?it/s]#015100% 50/50 [00:00<00:00, 50.52it/s, avg_epoch_loss=6.7]\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [INFO] root Epoch[1] Elapsed time 0.992 seconds\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [INFO] root Epoch[1] Evaluation metric 'epoch_loss'=6.703874\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [INFO] root Loading parameters from best epoch (1)\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [INFO] root Final loss: 6.7038740634918215 (occurred at epoch 1)\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [INFO] root End model training\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [WARNING] root Serializing RepresentableBlockPredictor instances does not save the prediction network structure in a backwards-compatible manner. Be careful not to use this method in production.\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:07] [INFO] __main__ Starting model evaluation.\u001b[0m\n",
      "\u001b[34m#015Running evaluation:   0% 0/2 [00:00<?, ?it/s]#015Running evaluation: 100% 2/2 [00:02<00:00,  1.14s/it]\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-MSE]: 54747.19140625\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-abs_error]: 713.1560897827148\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-abs_target_sum]: 4500.0\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-abs_target_mean]: 1125.0\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-seasonal_error]: 104.0\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-MASE]: 1.5005334726264399\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-sMAPE]: 1.0692570890980746\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-MSIS]: 6.7701040737090565\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.1]]: 228.673876953125\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.1]]: 0.0\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.2]]: 320.0845671653748\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.2]]: 0.0\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.3]]: 504.6300064086914\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.3]]: 0.5\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.4]]: 647.3801239013671\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.4]]: 0.5\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.5]]: 713.1560883522034\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.5]]: 0.5\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.6]]: 725.7405807495117\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.6]]: 0.75\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.7]]: 675.7963920593262\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.7]]: 0.75\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.8]]: 491.3031951904296\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.8]]: 0.75\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-QuantileLoss[0.9]]: 216.0477615356445\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-Coverage[0.9]]: 0.75\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-RMSE]: 233.98117746145735\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-NRMSE]: 0.20798326885462876\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-ND]: 0.15847913106282552\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.1]]: 0.05081641710069444\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.2]]: 0.07112990381452773\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.3]]: 0.11214000142415363\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.4]]: 0.14386224975585937\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.5]]: 0.15847913074493408\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.6]]: 0.16127568461100258\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.7]]: 0.1501769760131836\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.8]]: 0.10917848782009547\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wQuantileLoss[0.9]]: 0.048010613674587664\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-mean_wQuantileLoss]: 0.11167438499544874\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-MAE_Coverage]: 0.11111111111111113\u001b[0m\n",
      "\u001b[34m[2020-04-23 02:49:10] [INFO] __main__ gluonts[metric-wMAPE]: 0.13260816037654877\u001b[0m\n",
      "\u001b[34m2020-04-23 02:49:10,263 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-04-23 02:49:20 Uploading - Uploading generated training model\n",
      "2020-04-23 02:49:20 Completed - Training job completed\n",
      "Training seconds: 70\n",
      "Billable seconds: 70\n"
     ]
    }
   ],
   "source": [
    "# Equivalent to:\n",
    "#\n",
    "# python entrypoint.py \\\n",
    "#     --s3-dataset <local_path for direct invocation> \\\n",
    "#     --plot_transparent 0 \\\n",
    "#     --num_samples 1000 \\\n",
    "#     --algo 'gluonts.model.deepar.DeepAREstimator' \\\n",
    "#     --use_feat_static_cat True \\\n",
    "#     --cardinality <json string in cardinality variable> \\\n",
    "#     --prediction_length <fcast_len> \\\n",
    "#     --distr_output gluonts.distribution.gaussian.GaussianOutput \\\n",
    "#     --trainer gluonts.trainer.Trainer \\\n",
    "#     --trainer.epochs 10\n",
    "mxnet_estimator = MXNet(\n",
    "                    entry_point='entrypoint.py',\n",
    "                    source_dir=source_dir,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py3',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m5.large',\n",
    "                    sagemaker_session=sess,\n",
    "                    hyperparameters={\n",
    "                        # Let's start with non-algorithm hyperparameters\n",
    "                        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "                        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "\n",
    "                        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "                        # etc. See glounts.model packages for the list of available algorithms.\n",
    "                        #\n",
    "                        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "                        'algo': 'gluonts.model.deepar.DeepAREstimator',\n",
    "\n",
    "                        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "                        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "                        #\n",
    "                        # There're two types of kwargs hyperparameters:\n",
    "                        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "                        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "                        #   None, respectively.\n",
    "                        # - Custom classes, notably Trainer and distribution output.\n",
    "                        #   Note that time_feat is unsupported at this point in time.\n",
    "                        \n",
    "                        # Kwargs: Primitive python types.\n",
    "                        'use_feat_static_cat': 'True',\n",
    "                        'cardinality': cardinality,\n",
    "                        'prediction_length': fcast_length,\n",
    "\n",
    "                        # Equivalent to DeepAREstimator(..., distr_output=GaussianOutput(), ...)\n",
    "                        'distr_output.__class__': 'gluonts.distribution.gaussian.GaussianOutput',\n",
    "\n",
    "                        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "                        'trainer.__class__': 'gluonts.trainer.Trainer',\n",
    "                        'trainer.epochs': 10,\n",
    "                    },\n",
    "\n",
    "                    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "                    # we choose to capture only a few.\n",
    "                    metric_definitions=[\n",
    "                        {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "                        {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "                        {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "                        {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "                        {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "                    ],\n",
    ")\n",
    "\n",
    "mxnet_estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: MODEL_S3=s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/output/model.tar.gz\n",
      "env: OUTPUT_S3=s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/output/output.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_s3 = mxnet_estimator.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "output_s3 = os.path.join(mxnet_estimator.latest_training_job.describe()['OutputDataConfig']['S3OutputPath'], mxnet_estimator.latest_training_job.job_name, 'output/output.tar.gz')\n",
    "model_s3, output_s3\n",
    "%set_env MODEL_S3=$model_s3\n",
    "%set_env OUTPUT_S3=$output_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:green;font-weight:bold;font-size:250%\">IMPORTANT:</div>\n",
    "\n",
    "Please note the resulted `model_s3` and `output_s3` in the above cell.\n",
    "- You'll need the `model_s3` in the next notebook to perform Batch Transform.\n",
    "- You can download, extract and inspect the content of `output_s3` to observe the training output: metrics, and the forecast plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe training results\n",
    "\n",
    "As in any SageMaker training job, entrypoint script will generate two artifacts in the S3: `model.tar.gz` and `output.tar.gz`.\n",
    "\n",
    "The `model.tar.gz` contains the persisted model that can be used later on for inference.\n",
    "\n",
    "The `output.tar.gz` contains the following:\n",
    "- individual plot of each test timeseries\n",
    "- montage of plots of all test timeseries\n",
    "- backtest evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model artifacts s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/output/model.tar.gz:\n",
      "-rw-r--r--  0 0      0         674 Apr 23 10:49 prediction_net-network.json\n",
      "-rw-r--r--  0 0      0        2778 Apr 23 10:49 input_transform.json\n",
      "-rw-r--r--  0 0      0          51 Apr 23 10:49 type.txt\n",
      "-rw-r--r--  0 0      0          38 Apr 23 10:49 version.json\n",
      "-rw-r--r--  0 0      0         476 Apr 23 10:49 parameters.json\n",
      "-rw-r--r--  0 0      0      105665 Apr 23 10:49 prediction_net-0000.params\n",
      "\n",
      "Output s3://sagemaker-ap-southeast-1-484597657167/mxnet-training-2020-04-23-02-46-03-719/output/output.tar.gz:\n",
      "-rw-r--r--  0 0      0        1361 Apr 23 10:49 agg_metrics.json\n",
      "drwxr-xr-x  0 0      0           0 Apr 23 10:49 plots/\n",
      "drwxr-xr-x  0 0      0           0 Apr 23 10:49 plots/single/\n",
      "-rw-r--r--  0 0      0      122350 Apr 23 10:49 plots/single/000.png\n",
      "-rw-r--r--  0 0      0      155169 Apr 23 10:49 plots/single/001.png\n",
      "-rw-r--r--  0 0      0       60553 Apr 23 10:49 plots/plots.png\n",
      "-rw-r--r--  0 0      0        1011 Apr 23 10:49 item_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"\\nModel artifacts $MODEL_S3:\"\n",
    "aws s3 cp $MODEL_S3 - | tar -tzvf -\n",
    "\n",
    "echo -e \"\\nOutput $OUTPUT_S3:\"\n",
    "aws s3 cp $OUTPUT_S3 - | tar -tzvf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: HPO\n",
    "\n",
    "We provide a few more examples how to run the entrypoint scripts through SageMaker HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuning_job(objective_metric_name, estimator_hp, tuner_hp, metric, role, sess):\n",
    "    # FIXME: do not use global variable source_dir\n",
    "    estimator = MXNet(entry_point='entrypoint.py',\n",
    "                      source_dir=source_dir,\n",
    "                      framework_version='1.6.0',\n",
    "                      py_version='py3',\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m5.large',\n",
    "                      sagemaker_session=sess,\n",
    "                      hyperparameters=estimator_hp,\n",
    "                      metric_definitions=metric,\n",
    "    )\n",
    "\n",
    "    tuner = HyperparameterTuner(\n",
    "                estimator,\n",
    "                objective_metric_name,\n",
    "                tuner_hp,\n",
    "                metric,   # Also needed for custom algo. (i.e., entrypoint script).\n",
    "                objective_type='Minimize',\n",
    "                max_jobs=4,   # Hardcoded (for testing).\n",
    "                max_parallel_jobs=1)\n",
    "    return tuner\n",
    "\n",
    "def get_ts():\n",
    "    return strftime(\"%y%m%d-%H%M%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tuner for DeepAR\n",
    "\n",
    "\\[SFG17\\] Salinas, David, Valentin Flunkert, and Jan Gasthaus. “DeepAR: Probabilistic forecasting with autoregressive recurrent networks.” arXiv preprint arXiv:1704.04110 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to:\n",
    "# python entrypoint.py --s3-dataset s3_dir --distr_output gluonts.distribution.gaussian.GaussianOutput \\\n",
    "#     --use_feat_static_cat True --cardinality '[5]' --prediction_length 2 \\\n",
    "#     --trainer gluonts.trainer.Trainer --trainer.epochs 2\n",
    "tuner_deepar = create_tuning_job(\n",
    "    objective_metric_name='test:wmape',\n",
    "\n",
    "    # Fixed hyperparameters, i.e., same for all training jobs.\n",
    "    estimator_hp={\n",
    "        # Let's start with non-algorithm hyperparameters\n",
    "        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "\n",
    "        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "        # etc. See glounts.model packages for the list of available algorithms.\n",
    "        #\n",
    "        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "        'algo': 'gluonts.model.deepar.DeepAREstimator',\n",
    "\n",
    "        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "        #\n",
    "        # There're two types of kwargs hyperparameters:\n",
    "        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "        #   None, respectively.\n",
    "        # - Custom classes, notably Trainer and distribution output.\n",
    "        #   Note that time_feat is unsupported at this point in time.\n",
    "\n",
    "        # Kwargs: Primitive python types.\n",
    "        'use_feat_static_cat': 'True',\n",
    "        'cardinality': '[5]',\n",
    "        'prediction_length': 2,\n",
    "\n",
    "        # Kwargs: custom classes.\n",
    "        # Currently, this is implemented as a whitelist, and notably missing is for kwarg time_feat.\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., distr_output=GaussianOutput(), ...)\n",
    "        'distr_output': 'gluonts.distribution.gaussian.GaussianOutput',\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "        'trainer': 'gluonts.trainer.Trainer',\n",
    "        'trainer.epochs': 10,\n",
    "    },\n",
    "\n",
    "    # Tunable hyperparameters, i.e., may vary across training jobs.\n",
    "    tuner_hp={\n",
    "        \"num_cells\": IntegerParameter(30, 200),\n",
    "        \"num_layers\": IntegerParameter(1, 8),\n",
    "        \"trainer.learning_rate\": ContinuousParameter(1e-5, 1e-1, scaling_type='Logarithmic'),\n",
    "    },\n",
    "\n",
    "    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "    # we choose to capture only a few.\n",
    "    metric=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "        {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "        {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "    ],\n",
    "\n",
    "    role=role,\n",
    "    sess=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tuner for DeepState\n",
    "\n",
    "\\[RSG+18\\] Rangapuram, Syama Sundar, et al. “Deep state space models for time series forecasting.” Advances in Neural Information Processing Systems. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to:\n",
    "# python entrypoint.py --s3_dataset s3_dir --algo gluonts.model.deepstate.DeepStateEstimator \\\n",
    "#    --trainer gluonts.trainer.Trainer --trainer.epochs 3 --use_feat_static_cat True --cardinality '[5]' \\\n",
    "#    --noise_std_bounds gluonts.distribution.lds.ParameterBounds --noise_std_bounds.lower 1e-5 --noise_std_bounds.upper 1e-1\n",
    "tuner_deepstate = create_tuning_job(\n",
    "    objective_metric_name='test:wmape',\n",
    "\n",
    "    # Fixed hyperparameters, i.e., same for all training jobs.\n",
    "    estimator_hp={\n",
    "        # Let's start with non-algorithm hyperparameters\n",
    "        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "\n",
    "        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "        # etc. See glounts.model packages for the list of available algorithms.\n",
    "        #\n",
    "        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "        'algo': 'gluonts.model.deepstate.DeepStateEstimator',\n",
    "\n",
    "        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "        #\n",
    "        # There're two types of kwargs hyperparameters:\n",
    "        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "        #   None, respectively.\n",
    "        # - Custom classes, notably Trainer and distribution output.\n",
    "        #   Note that time_feat is unsupported at this point in time.\n",
    "\n",
    "        # Kwargs: Primitive python types.\n",
    "        'use_feat_static_cat': 'True',\n",
    "        'cardinality': '[5]',\n",
    "        'prediction_length': 2,\n",
    "\n",
    "        # Kwargs: custom classes.\n",
    "        # Currently, this is implemented as a whitelist, and notably missing is for kwarg time_feat.\n",
    "\n",
    "        # Equivalent to DeepStateEstimator(..., noise_std_bound=ParameterBounds(lower=1e-06, upper=1.0), ...)\n",
    "        # This bounds are exactly the default, so you can choose not to specify the hyperparameters.\n",
    "        # However, we specify here to show you how you can customze the bounds. Please consult the\n",
    "        # DeepState documentations all the bounds it supports.\n",
    "        \"noise_std_bounds\": \"gluonts.distribution.lds.ParameterBounds\",\n",
    "        \"noise_std_bounds.lower\": \"1e-5\",\n",
    "        \"noise_std_bounds.upper\": \"1e-1\",\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "        'trainer': 'gluonts.trainer.Trainer',\n",
    "        'trainer.epochs': 10,\n",
    "    },\n",
    "\n",
    "    # Tunable hyperparameters, i.e., may vary across training jobs.\n",
    "    tuner_hp={\n",
    "        \"num_cells\": IntegerParameter(30, 200),\n",
    "        \"num_layers\": IntegerParameter(1, 8),\n",
    "        \"trainer.learning_rate\": ContinuousParameter(1e-5, 1e-1, scaling_type='Logarithmic'),\n",
    "    },\n",
    "\n",
    "    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "    # we choose to capture only a few.\n",
    "    metric=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "        {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "        {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "    ],\n",
    "\n",
    "    role=role,\n",
    "    sess=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start each tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gtsdeepar200423-052910\n",
      "gtsdeepstate200423-052911\n"
     ]
    }
   ],
   "source": [
    "tuner_deepar.fit(data_channels, job_name='gtsdeepar-'+get_ts(), include_cls_metadata=False)\n",
    "tuner_deepstate.fit(data_channels, job_name='gtsdeepstate-'+get_ts(), include_cls_metadata=False)\n",
    "\n",
    "# Show tuning names\n",
    "for tuner in [tuner_deepar, tuner_deepstate]:\n",
    "    print(tuner.latest_tuning_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until all tuning job finishes (check from console), then proceed with belows.\n",
    "\n",
    "**<div style=\"color:firebrick\">If you start a restart this notebook kernel, then you need to manually attach to an existing tuning job.\n",
    "Sample code shown below, but be aware that you need to manually specify the tuning jobs to attach!</div>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is a raw cell. To run this cell, first change to code cell, then execute.\n",
    "\n",
    "completed_tuner_names = [\n",
    "    'gtsdeepar200423-052910',\n",
    "    'gtsdeepstate200423-052911',\n",
    "]\n",
    "\n",
    "completed_tuners = [HyperparameterTuner.attach(tuner_job_name) for tuner_job_name in completed_tuner_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "DONE\n",
      "gtsdeepar200423-052910\n",
      "gtsdeepar200423-052910-004-0fd96b63\n",
      "test:wmape\n",
      "0.04612693935632706\n",
      "\n",
      "\n",
      "NOT_DONE\n",
      "gtsdeepstate200423-052911\n",
      "gtsdeepstate200423-052911-003-215cce67\n",
      "test:wmape\n",
      "0.05560236796736717\n"
     ]
    }
   ],
   "source": [
    "#for tuner in completed_tuners:\n",
    "for tuner in [tuner_deepar, tuner_deepstate]:\n",
    "    # Ugh, why must we travel all down the way to botocore level just to query tuning status..\n",
    "    if int(tuner.sagemaker_session.boto_session.client('sagemaker').describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.name)['ObjectiveStatusCounters']['Pending']) > 0:\n",
    "        status = 'NOT_DONE'\n",
    "    else:\n",
    "        status = 'DONE'\n",
    "\n",
    "    try:\n",
    "        best_training_job = tuner.best_training_job()\n",
    "    except:\n",
    "        # Exception: Best training job not available for tuning job: gtsdeepar-200423-064644\n",
    "        best_training_job = None\n",
    "\n",
    "    print('\\n',\n",
    "        status,\n",
    "        tuner.latest_tuning_job.name,\n",
    "        best_training_job,\n",
    "        #sess.sagemaker_client.describe_training_job(TrainingJobName=tuner.best_training_job())['HyperParameters']['likelihood'],\n",
    "        tuner.objective_metric_name,\n",
    "        tuner.analytics().dataframe()['FinalObjectiveValue'].min(),\n",
    "          \n",
    "        sep='\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
