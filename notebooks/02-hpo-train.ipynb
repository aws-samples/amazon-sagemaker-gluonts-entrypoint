{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div style='font-size:200%'>Minimalistic example for gluonts entrypoint script</div>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from time import gmtime, strftime \n",
    "\n",
    "import boto3\n",
    "import sagemaker as sm\n",
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner, IntegerParameter\n",
    "\n",
    "# A few standard SageMaker's stanzas\n",
    "role: str = get_sm_execution_role()\n",
    "sess = sm.Session()\n",
    "region: str = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data for training. Ensure the same as bucket, prefix, and dataset_name from 01-create-dataset.ipynb.\n",
    "bucket = 'BUCKET'\n",
    "prefix = 'PREFIX/DATASET_NAME'   # Ensure no trailing '/'.\n",
    "data_channels = {'s3_dataset': f's3://{bucket}/{prefix}'}\n",
    "print(data_channels)\n",
    "\n",
    "# Model hyperparameters. Here we demonstrate that:\n",
    "# - the dataset embeds its recommended forecast length (=12) in its metadata.\n",
    "# - However, each train job is free to override the forecast length. We override to 6 weeks.\n",
    "fcast_length = 6\n",
    "\n",
    "source_dir = '../src/entrypoint/'\n",
    "\n",
    "%set_env BUCKET=$bucket\n",
    "%set_env PREFIX=$prefix\n",
    "\n",
    "# Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "# we choose to capture only a few.\n",
    "metric=[\n",
    "    {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "    {\"Name\": \"test:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'validation_epoch_loss'=(\\S+)\"},\n",
    "    {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "    {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read cardinality of static feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF this cell fails, upgrade s3fs.\n",
    "with Path2(f's3://{bucket}/{prefix}/metadata/metadata.json').open('rb') as f:\n",
    "    cardinality = [int(json.load(f)['feat_static_cat'][0]['cardinality'])]\n",
    "cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave as an exercise to read the recommended forecast length in the dataset's metadata in `f's3://{bucket}/{prefix}/metadata/metadata.json'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same entrypoint script supports the various estimators in gluonts. You specify the estimator to the entrypoint script using hyperparameter `algo`. The next cell demonstrate the DeepAR estimator `gluonts.model.deepar.DeepAREstimator`.\n",
    "\n",
    "Feel free to experiment with another estimator e.g., `gluonts.model.deepstate.DeepStateEstimator`.\n",
    "\n",
    "Do note that you must consult the estimator's documentation for their hyperparameters aka. `kwargs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to:\n",
    "#\n",
    "# python entrypoint.py \\\n",
    "#     --s3-dataset <local_path for direct invocation> \\\n",
    "#     --plot_transparent 0 \\\n",
    "#     --num_samples 1000 \\\n",
    "#     --algo 'gluonts.model.deepar.DeepAREstimator' \\\n",
    "#     --use_feat_static_cat True \\\n",
    "#     --cardinality <json string in cardinality variable> \\\n",
    "#     --prediction_length <fcast_len> \\\n",
    "#     --distr_output gluonts.distribution.gaussian.GaussianOutput \\\n",
    "#     --trainer gluonts.trainer.Trainer \\\n",
    "#     --trainer.epochs 10\n",
    "mxnet_estimator = MXNet(\n",
    "                    entry_point='entrypoint.py',\n",
    "                    source_dir=source_dir,\n",
    "                    framework_version='1.6.0',\n",
    "                    py_version='py3',\n",
    "                    role=role,\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type='ml.m5.large',\n",
    "                    sagemaker_session=sess,\n",
    "                    hyperparameters={\n",
    "                        # Let's start with non-algorithm hyperparameters\n",
    "                        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "                        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "\n",
    "                        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "                        # etc. See glounts.model packages for the list of available algorithms.\n",
    "                        #\n",
    "                        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "                        'algo': 'gluonts.model.deepar.DeepAREstimator',\n",
    "\n",
    "                        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "                        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "                        #\n",
    "                        # There're two types of kwargs hyperparameters:\n",
    "                        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "                        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "                        #   None, respectively.\n",
    "                        # - Custom classes, notably Trainer and distribution output.\n",
    "                        #   Note that time_feat is unsupported at this point in time.\n",
    "                        \n",
    "                        # Kwargs: Primitive python types.\n",
    "                        'use_feat_static_cat': 'True',\n",
    "                        'cardinality': cardinality,\n",
    "                        'prediction_length': fcast_length,\n",
    "\n",
    "                        # Equivalent to DeepAREstimator(..., distr_output=GaussianOutput(), ...)\n",
    "                        'distr_output.__class__': 'gluonts.distribution.gaussian.GaussianOutput',\n",
    "\n",
    "                        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "                        'trainer.__class__': 'gluonts.trainer.Trainer',\n",
    "                        'trainer.epochs': 10,\n",
    "                    },\n",
    "\n",
    "                    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "                    # we choose to capture only a few.\n",
    "                    metric_definitions=[\n",
    "                        {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "                        {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "                        {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "                        {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "                        {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "                    ],\n",
    ")\n",
    "\n",
    "mxnet_estimator.fit(data_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3 = mxnet_estimator.latest_training_job.describe()['ModelArtifacts']['S3ModelArtifacts']\n",
    "output_s3 = os.path.join(mxnet_estimator.latest_training_job.describe()['OutputDataConfig']['S3OutputPath'], mxnet_estimator.latest_training_job.job_name, 'output/output.tar.gz')\n",
    "model_s3, output_s3\n",
    "%set_env MODEL_S3=$model_s3\n",
    "%set_env OUTPUT_S3=$output_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color:green;font-weight:bold;font-size:250%\">IMPORTANT:</div>\n",
    "\n",
    "Please note the resulted `model_s3` and `output_s3` in the above cell.\n",
    "- You'll need the `model_s3` in the next notebook to perform Batch Transform.\n",
    "- You can download, extract and inspect the content of `output_s3` to observe the training output: metrics, and the forecast plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe training results\n",
    "\n",
    "As in any SageMaker training job, entrypoint script will generate two artifacts in the S3: `model.tar.gz` and `output.tar.gz`.\n",
    "\n",
    "The `model.tar.gz` contains the persisted model that can be used later on for inference.\n",
    "\n",
    "The `output.tar.gz` contains the following:\n",
    "- individual plot of each test timeseries\n",
    "- montage of plots of all test timeseries\n",
    "- backtest evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"\\nModel artifacts $MODEL_S3:\"\n",
    "aws s3 cp $MODEL_S3 - | tar -tzvf -\n",
    "\n",
    "echo -e \"\\nOutput $OUTPUT_S3:\"\n",
    "aws s3 cp $OUTPUT_S3 - | tar -tzvf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional: HPO\n",
    "\n",
    "We provide a few more examples how to run the entrypoint scripts through SageMaker HPO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuning_job(objective_metric_name, estimator_hp, tuner_hp, metric, role, sess):\n",
    "    # FIXME: do not use global variable source_dir\n",
    "    estimator = MXNet(entry_point='entrypoint.py',\n",
    "                      source_dir=source_dir,\n",
    "                      framework_version='1.6.0',\n",
    "                      py_version='py3',\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m5.large',\n",
    "                      sagemaker_session=sess,\n",
    "                      hyperparameters=estimator_hp,\n",
    "                      metric_definitions=metric,\n",
    "    )\n",
    "\n",
    "    tuner = HyperparameterTuner(\n",
    "                estimator,\n",
    "                objective_metric_name,\n",
    "                tuner_hp,\n",
    "                metric,   # Also needed for custom algo. (i.e., entrypoint script).\n",
    "                objective_type='Minimize',\n",
    "                max_jobs=4,   # Hardcoded (for testing).\n",
    "                max_parallel_jobs=1)\n",
    "    return tuner\n",
    "\n",
    "def get_ts():\n",
    "    return strftime(\"%y%m%d-%H%M%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tuner for DeepAR\n",
    "\n",
    "\\[SFG17\\] Salinas, David, Valentin Flunkert, and Jan Gasthaus. “DeepAR: Probabilistic forecasting with autoregressive recurrent networks.” arXiv preprint arXiv:1704.04110 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to:\n",
    "# python entrypoint.py --s3-dataset s3_dir --distr_output gluonts.distribution.gaussian.GaussianOutput \\\n",
    "#     --use_feat_static_cat True --cardinality '[5]' --prediction_length 2 \\\n",
    "#     --trainer gluonts.trainer.Trainer --trainer.epochs 2\n",
    "tuner_deepar = create_tuning_job(\n",
    "    objective_metric_name='test:wmape',\n",
    "\n",
    "    # Fixed hyperparameters, i.e., same for all training jobs.\n",
    "    estimator_hp={\n",
    "        # Let's start with non-algorithm hyperparameters\n",
    "        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "\n",
    "        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "        # etc. See glounts.model packages for the list of available algorithms.\n",
    "        #\n",
    "        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "        'algo': 'gluonts.model.deepar.DeepAREstimator',\n",
    "\n",
    "        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "        #\n",
    "        # There're two types of kwargs hyperparameters:\n",
    "        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "        #   None, respectively.\n",
    "        # - Custom classes, notably Trainer and distribution output.\n",
    "        #   Note that time_feat is unsupported at this point in time.\n",
    "\n",
    "        # Kwargs: Primitive python types.\n",
    "        'use_feat_static_cat': 'True',\n",
    "        'cardinality': '[5]',\n",
    "        'prediction_length': 2,\n",
    "\n",
    "        # Kwargs: custom classes.\n",
    "        # Currently, this is implemented as a whitelist, and notably missing is for kwarg time_feat.\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., distr_output=GaussianOutput(), ...)\n",
    "        'distr_output': 'gluonts.distribution.gaussian.GaussianOutput',\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "        'trainer': 'gluonts.trainer.Trainer',\n",
    "        'trainer.epochs': 10,\n",
    "    },\n",
    "\n",
    "    # Tunable hyperparameters, i.e., may vary across training jobs.\n",
    "    tuner_hp={\n",
    "        \"num_cells\": IntegerParameter(30, 200),\n",
    "        \"num_layers\": IntegerParameter(1, 8),\n",
    "        \"trainer.learning_rate\": ContinuousParameter(1e-5, 1e-1, scaling_type='Logarithmic'),\n",
    "    },\n",
    "\n",
    "    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "    # we choose to capture only a few.\n",
    "    metric=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "        {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "        {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "    ],\n",
    "\n",
    "    role=role,\n",
    "    sess=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tuner for DeepState\n",
    "\n",
    "\\[RSG+18\\] Rangapuram, Syama Sundar, et al. “Deep state space models for time series forecasting.” Advances in Neural Information Processing Systems. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalent to:\n",
    "# python entrypoint.py --s3_dataset s3_dir --algo gluonts.model.deepstate.DeepStateEstimator \\\n",
    "#    --trainer gluonts.trainer.Trainer --trainer.epochs 3 --use_feat_static_cat True --cardinality '[5]' \\\n",
    "#    --noise_std_bounds gluonts.distribution.lds.ParameterBounds --noise_std_bounds.lower 1e-5 --noise_std_bounds.upper 1e-1\n",
    "tuner_deepstate = create_tuning_job(\n",
    "    objective_metric_name='test:wmape',\n",
    "\n",
    "    # Fixed hyperparameters, i.e., same for all training jobs.\n",
    "    estimator_hp={\n",
    "        # Let's start with non-algorithm hyperparameters\n",
    "        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "\n",
    "        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "        # etc. See glounts.model packages for the list of available algorithms.\n",
    "        #\n",
    "        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "        'algo': 'gluonts.model.deepstate.DeepStateEstimator',\n",
    "\n",
    "        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "        #\n",
    "        # There're two types of kwargs hyperparameters:\n",
    "        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "        #   None, respectively.\n",
    "        # - Custom classes, notably Trainer and distribution output.\n",
    "        #   Note that time_feat is unsupported at this point in time.\n",
    "\n",
    "        # Kwargs: Primitive python types.\n",
    "        'use_feat_static_cat': 'True',\n",
    "        'cardinality': '[5]',\n",
    "        'prediction_length': 2,\n",
    "\n",
    "        # Kwargs: custom classes.\n",
    "        # Currently, this is implemented as a whitelist, and notably missing is for kwarg time_feat.\n",
    "\n",
    "        # Equivalent to DeepStateEstimator(..., noise_std_bound=ParameterBounds(lower=1e-06, upper=1.0), ...)\n",
    "        # This bounds are exactly the default, so you can choose not to specify the hyperparameters.\n",
    "        # However, we specify here to show you how you can customze the bounds. Please consult the\n",
    "        # DeepState documentations all the bounds it supports.\n",
    "        \"noise_std_bounds\": \"gluonts.distribution.lds.ParameterBounds\",\n",
    "        \"noise_std_bounds.lower\": \"1e-5\",\n",
    "        \"noise_std_bounds.upper\": \"1e-1\",\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "        'trainer': 'gluonts.trainer.Trainer',\n",
    "        'trainer.epochs': 10,\n",
    "    },\n",
    "\n",
    "    # Tunable hyperparameters, i.e., may vary across training jobs.\n",
    "    tuner_hp={\n",
    "        \"num_cells\": IntegerParameter(30, 200),\n",
    "        \"num_layers\": IntegerParameter(1, 8),\n",
    "        \"trainer.learning_rate\": ContinuousParameter(1e-5, 1e-1, scaling_type='Logarithmic'),\n",
    "    },\n",
    "\n",
    "    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "    # we choose to capture only a few.\n",
    "    metric=[\n",
    "        {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "        {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "        {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "        {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "    ],\n",
    "\n",
    "    role=role,\n",
    "    sess=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start each tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_deepar.fit(data_channels, job_name='gtsdeepar-'+get_ts(), include_cls_metadata=False)\n",
    "tuner_deepstate.fit(data_channels, job_name='gtsdeepstate-'+get_ts(), include_cls_metadata=False)\n",
    "\n",
    "# Show tuning names\n",
    "for tuner in [tuner_deepar, tuner_deepstate]:\n",
    "    print(tuner.latest_tuning_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait until all tuning job finishes (check from console), then proceed with belows.\n",
    "\n",
    "**<div style=\"color:firebrick\">If you start a restart this notebook kernel, then you need to manually attach to an existing tuning job.\n",
    "Sample code shown below, but be aware that you need to manually specify the tuning jobs to attach!</div>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is a raw cell. To run this cell, first change to code cell, then execute.\n",
    "\n",
    "completed_tuner_names = [\n",
    "    'gtsdeepar200423-052910',\n",
    "    'gtsdeepstate200423-052911',\n",
    "]\n",
    "\n",
    "completed_tuners = [HyperparameterTuner.attach(tuner_job_name) for tuner_job_name in completed_tuner_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tuner in completed_tuners:\n",
    "for tuner in [tuner_deepar, tuner_deepstate]:\n",
    "    # Ugh, why must we travel all down the way to botocore level just to query tuning status..\n",
    "    if int(tuner.sagemaker_session.boto_session.client('sagemaker').describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuner.latest_tuning_job.name)['ObjectiveStatusCounters']['Pending']) > 0:\n",
    "        status = 'NOT_DONE'\n",
    "    else:\n",
    "        status = 'DONE'\n",
    "\n",
    "    try:\n",
    "        best_training_job = tuner.best_training_job()\n",
    "    except:\n",
    "        # Exception: Best training job not available for tuning job: gtsdeepar-200423-064644\n",
    "        best_training_job = None\n",
    "\n",
    "    print('\\n',\n",
    "        status,\n",
    "        tuner.latest_tuning_job.name,\n",
    "        best_training_job,\n",
    "        #sess.sagemaker_client.describe_training_job(TrainingJobName=tuner.best_training_job())['HyperParameters']['likelihood'],\n",
    "        tuner.objective_metric_name,\n",
    "        tuner.analytics().dataframe()['FinalObjectiveValue'].min(),\n",
    "          \n",
    "        sep='\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
