{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div style='font-size:200%'>Minimalistic example to train using gluonts meta-entrypoint script</div>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import json\n",
    "from time import gmtime, strftime\n",
    "\n",
    "import boto3\n",
    "import sagemaker as sm\n",
    "from sagemaker.mxnet.estimator import MXNet\n",
    "from sagemaker.tuner import CategoricalParameter, ContinuousParameter, HyperparameterTuner, IntegerParameter\n",
    "from smallmatter.pathlib import Path2\n",
    "from smallmatter.sm import get_sm_execution_role\n",
    "\n",
    "# smallmatter.sm.get_sm_execution_role() will:\n",
    "# - on SageMaker classic notebook instance, simply call sagemaker.get_execution_role()\n",
    "# - outside of SageMaker classic notebook instance, return the first role whose name\n",
    "#   startswith \"AmazonSageMaker-ExecutionRole-\"\n",
    "role: str = get_sm_execution_role()\n",
    "\n",
    "sess = sm.Session()\n",
    "region: str = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global config\n",
    "\n",
    "NOTE: you may want to reduce the value of `epochs` for quicker tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data for training. Ensure the same as bucket, prefix, and dataset_name from 01-convert-csv-to-gluonts-format.ipynb.\n",
    "bucket = 'BUCKETNAME'\n",
    "prefix = 'gluonts-examples-dataset/synthetic-dataset'   # Ensure no trailing '/'.\n",
    "data_channels = {'s3_dataset': f's3://{bucket}/{prefix}'}\n",
    "print(data_channels)\n",
    "\n",
    "# Export S3 path as environment variable for cells that run shell commands.\n",
    "%set_env BUCKET=$bucket\n",
    "%set_env PREFIX=$prefix\n",
    "\n",
    "# Location of entrypoint train script.\n",
    "source_dir = '../src/entrypoint/'\n",
    "\n",
    "# Model hyperparameters.\n",
    "epochs = 200\n",
    "\n",
    "# Forecast length -- demonstrate that:\n",
    "# - the dataset embeds its recommended forecast length (=30 days) in its metadata.\n",
    "# - However, each train job is free to override the forecast length by changing\n",
    "#   fcast_length to a different value.\n",
    "fcast_length = 30\n",
    "\n",
    "# Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "# we choose to capture only a few.\n",
    "metric=[\n",
    "    {\"Name\": \"train:loss\", \"Regex\": r\"Epoch\\[\\d+\\] Evaluation metric 'epoch_loss'=(\\S+)\"},\n",
    "    {\"Name\": \"train:learning_rate\", \"Regex\": r\"Epoch\\[\\d+\\] Learning rate is (\\S+)\"},\n",
    "    {\"Name\": \"test:abs_error\", \"Regex\": r\"gluonts\\[metric-abs_error\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:rmse\", \"Regex\": r\"gluonts\\[metric-RMSE\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:mape\", \"Regex\": r\"gluonts\\[metric-MAPE\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:smape\", \"Regex\": r\"gluonts\\[metric-sMAPE\\]: (\\S+)\"},\n",
    "    {\"Name\": \"test:wmape\", \"Regex\": r\"gluonts\\[metric-wMAPE\\]: (\\S+)\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read cardinality of static feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this cell fails, then please upgrade s3fs.\n",
    "with Path2(f's3://{bucket}/{prefix}/metadata/metadata.json').open('rb') as f:\n",
    "    cardinality = [int(json.load(f)['feat_static_cat'][0]['cardinality'])]\n",
    "cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave as an exercise to read the recommended forecast length in the dataset's metadata in `f's3://{bucket}/{prefix}/metadata/metadata.json'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same `train.py` is a meta-entrypoint script, as it supports various gluonts estimators and their specific hyperparameters.\n",
    "\n",
    "To run the meta-entrypoint script as a SageMaker training job, two key informations are needed:\n",
    "\n",
    "1. Set hyperparameter `algo` to `gluonts.model.deepar.DeepAREstimator` (i.e., the fully-qualified class name of the DeepAR estimator).\n",
    "2. Set additional hyperparameters that correspond to the estimator's\n",
    "   [kwargs](https://ts.gluon.ai/api/gluonts/gluonts.model.deepar.html#module-gluonts.model.deepar).\n",
    "\n",
    "Feel free to experiment with another estimator e.g., `gluonts.model.deepstate.DeepStateEstimator`.\n",
    "\n",
    "**TIPS**: remember to always consult the estimator's documentation for their hyperparameters aka. `kwargs`,\n",
    "especially when experimenting with a different estimator or a different version of gluonts.\n",
    "\n",
    "> Some well-known cases:\n",
    "> - hyperparameter `trainer.__class__`, which corresponds to the `trainer` kwarg, was changed from\n",
    ">   `gluonts.trainer.Trainer` (gluonts-0.5) to `gluonts.mx.trainer.Trainer` (gluonts-0.8).\n",
    "> - hyperparameter `distr_output.__class__`, which corresponds to the `distr_output` kwarg, was\n",
    ">   changed from `gluonts.distribution.gaussian.GaussianOutput` (gluonts-0.5) to\n",
    ">   `gluonts.mx.distribution.GaussianOutput` (gluonts-0.8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell submits an MXNet estimator to run as a SageMaker training job.\n",
    "#\n",
    "# It is equivalent to directly running the train.py script as:\n",
    "#\n",
    "# python train.py \\\n",
    "#     --plot_transparent 0 \\\n",
    "#     --num_samples 1000 \\\n",
    "#     --algo 'gluonts.model.deepar.DeepAREstimator' \\\n",
    "#     --use_feat_static_cat True \\\n",
    "#     --cardinality <json string in cardinality variable> \\\n",
    "#     --prediction_length <fcast_len> \\\n",
    "#     --cell_type gru \\\n",
    "#     --distr_output.__class__ gluonts.mx.distribution.GaussianOutput \\\n",
    "#     --trainer.__class__ gluonts.mx.trainer.Trainer \\\n",
    "#     --trainer.epochs 200 \\\n",
    "#     --s3_dataset /opt/ml/input/s3_dataset \\\n",
    "mxnet_estimator = MXNet(\n",
    "                    entry_point='train.py',\n",
    "                    source_dir=source_dir,\n",
    "                    framework_version='1.7.0',\n",
    "                    py_version='py3',\n",
    "                    role=role,\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.2xlarge',\n",
    "                    sagemaker_session=sess,\n",
    "                    hyperparameters={\n",
    "                        # Let's start with non-algorithm hyperparameters\n",
    "                        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "                        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "                        #'y_transform': 'log1p',  # Uncomment to train model with log-transformed targets.\n",
    "\n",
    "                        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "                        # etc. See glounts.model packages for the list of available algorithms.\n",
    "                        #\n",
    "                        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "                        'algo': 'gluonts.model.deepar.DeepAREstimator',\n",
    "\n",
    "                        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "                        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "                        #\n",
    "                        # There're two types of kwargs hyperparameters:\n",
    "                        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "                        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "                        #   None, respectively.\n",
    "                        # - Custom classes, notably Trainer and distribution output.\n",
    "                        #   Note that time_feat is unsupported at this point in time.\n",
    "                        \n",
    "                        # Kwargs: Primitive python types.\n",
    "                        'use_feat_static_cat': 'True',\n",
    "                        'cardinality': cardinality,\n",
    "                        'prediction_length': fcast_length,\n",
    "                        'cell_type': 'gru',\n",
    "\n",
    "                        # Equivalent to DeepAREstimator(..., distr_output=GaussianOutput(), ...)\n",
    "                        'distr_output.__class__': 'gluonts.mx.distribution.GaussianOutput',\n",
    "\n",
    "                        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=...), ...)\n",
    "                        'trainer.__class__': 'gluonts.mx.trainer.Trainer',\n",
    "                        'trainer.epochs': epochs,\n",
    "                    },\n",
    "\n",
    "                    # Metric emitted by each training job. The entrypoint script may emit even more metrics,\n",
    "                    # but we choose to capture only a few.\n",
    "                    metric_definitions=metric,\n",
    ")\n",
    " \n",
    "# Setting wait=False frees this notebook from getting blocked by the training job.\n",
    "mxnet_estimator.fit(data_channels, wait=False)\n",
    "\n",
    "# Display reminder.\n",
    "print()\n",
    "print('Training job name:', mxnet_estimator.latest_training_job.job_name)\n",
    "print()\n",
    "print('While the training job runs, you can safely shutdown this notebook kernel and close this notebook,')\n",
    "print('and go to the SageMaker console to monitor the training progress. The training job\\'s console')\n",
    "print('also contains links to the CloudWatch log.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep track the expected S3 locations of model and output artifacts. Once the training job completes succesfully, the files will be available in these locations.\n",
    "\n",
    "While the training job is running, you may shutdown this notebook's kernel, close this notebook, and go to the SageMaker console to monitor the training progress. The training job's console also contains links to CloudWatch log.\n",
    "\n",
    "<div style=\"color:green;font-weight:bold;font-size:250%\">IMPORTANT:</div>\n",
    "\n",
    "Please note the *training job name* in the above cell. Once the training job completes, open the notebook `03-batch-transform.ipynb` to:\n",
    "\n",
    "1. Download, extract, and inspect the content of `output_s3` to observe the training output: metrics, and the forecast plots.\n",
    "\n",
    "2. Register the model artifact to Amazon Sagemaker, then perform Batch Transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optional, Advance Topics: HPO\n",
    "\n",
    "We provide a few more examples how to run the entrypoint scripts through SageMaker HPO using another gluonts's built-in algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tuning_job(objective_metric_name, estimator_hp, tuner_hp, metric, role, sess):\n",
    "    estimator = MXNet(entry_point='train.py',\n",
    "                      source_dir=source_dir,\n",
    "                      framework_version='1.7.0',\n",
    "                      py_version='py3',\n",
    "                      role=role,\n",
    "                      instance_count=1,\n",
    "                      instance_type='ml.m5.2xlarge',\n",
    "                      sagemaker_session=sess,\n",
    "                      hyperparameters=estimator_hp,\n",
    "                      metric_definitions=metric,\n",
    "    )\n",
    "\n",
    "    tuner = HyperparameterTuner(\n",
    "                estimator,\n",
    "                objective_metric_name,\n",
    "                tuner_hp,\n",
    "                metric,   # Also needed for custom algo. (i.e., entrypoint script).\n",
    "                objective_type='Minimize',\n",
    "                max_jobs=4,   # Hardcoded (for testing).\n",
    "                max_parallel_jobs=1)\n",
    "    return tuner\n",
    "\n",
    "def get_ts():\n",
    "    return strftime(\"%y%m%d-%H%M%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tuner for DeepAR\n",
    "\n",
    "\\[SFG17\\] Salinas, David, Valentin Flunkert, and Jan Gasthaus. “DeepAR: Probabilistic forecasting with autoregressive recurrent networks.” arXiv preprint arXiv:1704.04110 (2017)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_deepar = create_tuning_job(\n",
    "    objective_metric_name='test:wmape',\n",
    "\n",
    "    # Fixed hyperparameters, i.e., same for all training jobs.\n",
    "    estimator_hp={\n",
    "        # Let's start with non-algorithm hyperparameters\n",
    "        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "        #'y_transform': 'log1p',  # Uncomment to train model with log-transformed targets.\n",
    "\n",
    "        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "        # etc. See glounts.model packages for the list of available algorithms.\n",
    "        #\n",
    "        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "        'algo': 'gluonts.model.deepar.DeepAREstimator',\n",
    "\n",
    "        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepAR, consult the\n",
    "        # documentation for gluonts.model.deepar.DeepAREstimator.\n",
    "        #\n",
    "        # There're two types of kwargs hyperparameters:\n",
    "        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "        #   None, respectively.\n",
    "        # - Custom classes, notably Trainer and distribution output.\n",
    "        #   Note that time_feat is unsupported at this point in time.\n",
    "\n",
    "        # Kwargs: Primitive python types.\n",
    "        'use_feat_static_cat': 'True',\n",
    "        'cardinality': cardinality,\n",
    "        'prediction_length': fcast_length,\n",
    "        'cell_type': 'gru',\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., distr_output=GaussianOutput(), ...)\n",
    "        'distr_output.__class__': 'gluonts.mx.distribution.GaussianOutput',\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "        'trainer.__class__': 'gluonts.mx.trainer.Trainer',\n",
    "        'trainer.epochs': epochs,\n",
    "    },\n",
    "\n",
    "    # Tunable hyperparameters, i.e., may vary across training jobs.\n",
    "    tuner_hp={\n",
    "        \"num_cells\": IntegerParameter(30, 100),\n",
    "        \"num_layers\": IntegerParameter(2, 4),\n",
    "\n",
    "        # Minimum learning rate is 5e-5, which equals to the default `minimum_learning_rate`\n",
    "        # kwarg of the trainer. See: gluonts.mx.trainer.Trainer\n",
    "        \"trainer.learning_rate\": ContinuousParameter(5e-5, 1e-3, scaling_type='Logarithmic'),\n",
    "    },\n",
    "\n",
    "    # Metric emitted by each training job. The entrypoint script may emit even more metrics,\n",
    "    # but we choose to capture only a few.\n",
    "    metric=metric,\n",
    "\n",
    "    role=role,\n",
    "    sess=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a tuner for DeepState\n",
    "\n",
    "\\[RSG+18\\] Rangapuram, Syama Sundar, et al. “Deep state space models for time series forecasting.” Advances in Neural Information Processing Systems. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_deepstate = create_tuning_job(\n",
    "    objective_metric_name='test:wmape',\n",
    "\n",
    "    # Fixed hyperparameters, i.e., same for all training jobs.\n",
    "    estimator_hp={\n",
    "        # Let's start with non-algorithm hyperparameters\n",
    "        'plot_transparent': 0,   # Whether plot should be transparent or white background\n",
    "        'num_samples': 1000,     # Number of samples during backtesting.\n",
    "        #'y_transform': 'log1p',  # Uncomment to train model with log-transformed targets.\n",
    "\n",
    "        # Here, you specify the algorithm to use, such as DeepAR, DeepFactor, DeepState, Transformer,\n",
    "        # etc. See glounts.model packages for the list of available algorithms.\n",
    "        #\n",
    "        # If 'algo' is not specified, then defaults to 'gluonts.model.deepar.DeepAREstimator'.\n",
    "        'algo': 'gluonts.model.deepstate.DeepStateEstimator',\n",
    "\n",
    "        # The remaining here are kwargs to the chosen estimator. For e.g., for DeepState, consult the\n",
    "        # documentation for gluonts.model.deepstate.DeepStateEstimator.\n",
    "        #\n",
    "        # There're two types of kwargs hyperparameters:\n",
    "        # - primitive python types (incl. dictionaries & lists that can be deserialized from JSON).\n",
    "        #   Note that string \"True\", \"False\", and \"None\" will automatically become True, False, and\n",
    "        #   None, respectively.\n",
    "        # - Custom classes, notably Trainer and distribution output.\n",
    "        #   Note that time_feat is unsupported at this point in time.\n",
    "\n",
    "        # Kwargs: Primitive python types.\n",
    "        'use_feat_static_cat': 'True',\n",
    "        'cardinality': cardinality,\n",
    "        'prediction_length': fcast_length,\n",
    "        'cell_type': 'gru',\n",
    "\n",
    "        # Equivalent to DeepStateEstimator(..., noise_std_bound=ParameterBounds(lower=1e-06, upper=1.0), ...)\n",
    "        # This bounds are exactly the default, so you can choose not to specify the hyperparameters.\n",
    "        # However, we specify here to show you how you can customize the bounds. Please consult the\n",
    "        # DeepState documentations all the bounds it supports.\n",
    "        \"noise_std_bounds.__class__\": \"gluonts.mx.distribution.lds.ParameterBounds\",\n",
    "        \"noise_std_bounds.lower\": \"1e-6\",\n",
    "        \"noise_std_bounds.upper\": \"1e-1\",\n",
    "\n",
    "        # Equivalent to DeepAREstimator(..., trainer=Trainer(epochs=2), ...)\n",
    "        'trainer.__class__': 'gluonts.mx.trainer.Trainer',\n",
    "        'trainer.epochs': epochs,\n",
    "    },\n",
    "\n",
    "    # Tunable hyperparameters, i.e., may vary across training jobs.\n",
    "    tuner_hp={\n",
    "        \"num_cells\": IntegerParameter(30, 100),\n",
    "        \"num_layers\": IntegerParameter(2, 4),\n",
    "\n",
    "        # Minimum learning rate is 5e-5, which equals to the default `minimum_learning_rate`\n",
    "        # kwarg of the trainer. See: gluonts.mx.trainer.Trainer\n",
    "        \"trainer.learning_rate\": ContinuousParameter(5e-5, 1e-3, scaling_type='Logarithmic'),\n",
    "    },\n",
    "\n",
    "    # Metric emitted by each training job. The entrypoint script may emit even more metrics, but\n",
    "    # we choose to capture only a few.\n",
    "    metric=metric,\n",
    "\n",
    "    role=role,\n",
    "    sess=sess,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start each tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_deepar.fit(data_channels, job_name='gtsdeepar-'+get_ts(), include_cls_metadata=False)\n",
    "tuner_deepstate.fit(data_channels, job_name='gtsdeepstate-'+get_ts(), include_cls_metadata=False)\n",
    "\n",
    "# Show tuning names\n",
    "for tuner in [tuner_deepar, tuner_deepstate]:\n",
    "    print(tuner.latest_tuning_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, monitor the tuning jobs in the console until they complete.\n",
    "\n",
    "**<div style=\"color:firebrick\">NOTE: If you start a restart this notebook kernel, then you need run the next cell to attach to an existing tuning job.\n",
    "Sample code shown below. Be aware that you must update the cell with your tuning jobs before running it.</div>**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This is a raw cell. To run this cell, first change to code cell, update the tuner names, then execute.\n",
    "\n",
    "completed_tuner_names = [\n",
    "    'gtsdeepar-201029-135852',\n",
    "    'gtsdeepstate-201029-135852',\n",
    "]\n",
    "\n",
    "completed_tuners = [HyperparameterTuner.attach(tuner_job_name) for tuner_job_name in completed_tuner_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for tuner in completed_tuners:\n",
    "for tuner in [tuner_deepar, tuner_deepstate]:\n",
    "    # Travel all down the way to botocore level to query tuning status.\n",
    "    if int(\n",
    "            tuner\n",
    "               .sagemaker_session\n",
    "               .boto_session\n",
    "               .client('sagemaker')\n",
    "               .describe_hyper_parameter_tuning_job(\n",
    "                   HyperParameterTuningJobName=tuner.latest_tuning_job.name\n",
    "               )['ObjectiveStatusCounters']['Pending']\n",
    "    ) > 0:\n",
    "        status = 'NOT_DONE'\n",
    "    else:\n",
    "        status = 'DONE'\n",
    "\n",
    "    try:\n",
    "        best_training_job = tuner.best_training_job()\n",
    "    except:\n",
    "        # Exception: Best training job not available for tuning job: gtsdeepar-200423-064644\n",
    "        best_training_job = None\n",
    "\n",
    "    print('\\n',\n",
    "        status,\n",
    "        tuner.latest_tuning_job.name,\n",
    "        best_training_job,\n",
    "        #sess.sagemaker_client.describe_training_job(TrainingJobName=tuner.best_training_job())['HyperParameters']['likelihood'],\n",
    "        tuner.objective_metric_name,\n",
    "        tuner.analytics().dataframe()['FinalObjectiveValue'].min(),\n",
    "          \n",
    "        sep='\\n',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_latest_p37",
   "language": "python",
   "name": "conda_mxnet_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
