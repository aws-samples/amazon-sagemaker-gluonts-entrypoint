{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div style=\"font-size:200%\">Create gluonts DataSet</div>**\n",
    "\n",
    "This notebook demonstrates how to\n",
    "\n",
    "1. create a gluonts dataset which consists of train split, test split, and metadata.\n",
    "2. add label encodings as a custom metadata to the gluonts dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from gluonts.dataset.common import ListDataset, save_datasets, TrainDatasets, MetaData, CategoricalFeatureInfo, load_datasets\n",
    "\n",
    "#from smallmatter.common.pogr import fill_dt_all\n",
    "from smallmatter.ds import DFBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose weekly or daily.\n",
    "#freq, fcast_length = \"D\", 14\n",
    "freq, fcast_length = \"W\", 12\n",
    "\n",
    "dataset_name = f'gluonts-20200528-cat-{freq.lower()}'\n",
    "\n",
    "# Input\n",
    "pogr_csv = ['../../data/raw/Consolidated_PO_GR_AT14.csv',\n",
    "            '../../data/raw/Consolidated_PO_GR_AT16.csv']\n",
    "\n",
    "# Dates from consolidate_pogr to use.\n",
    "min_date = '2017-01-01'\n",
    "max_date = '2020-05-06'\n",
    "\n",
    "# Output\n",
    "bucket = 'OUTPUT_BUCKET'\n",
    "prefix = 'OUTPUT_PREFIX'\n",
    "\n",
    "%set_env BUCKET=$bucket\n",
    "%set_env PREFIX=$prefix\n",
    "%set_env DATASET_NAME=$dataset_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download .csv timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://archive.ics.uci.edu/ml/machine-learning-databases/00409/Daily_Demand_Forecasting_Orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Timeseries\n",
    "\n",
    "Our example dataset is from the [UCI Daily Demand Forecasting Orders Data Set](https://archive.ics.uci.edu/ml/datasets/Daily+Demand+Forecasting+Orders)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl https://archive.ics.uci.edu/ml/machine-learning-databases/00409/Daily_Demand_Forecasting_Orders.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = (po_gr[['goods_receipt_date', 'std_quantity', 'frequency', 'category']]\n",
    "          .reset_index(drop=True)\n",
    "          .rename({'goods_receipt_date': 'x', 'std_quantity': 'y', 'frequency': 'freq', 'category': 'cat'}, axis=1)\n",
    "     )\n",
    "ts = ts[(ts['x'] >= min_date) & (ts['x'] <= max_date)]\n",
    "\n",
    "# Track statistics\n",
    "stats = DFBuilder(columns=['stat', 'value'])\n",
    "stats += ['cat', ts['cat'].unique().shape[0]]\n",
    "display(stats.df)\n",
    "\n",
    "# IMPORTANT: consolidate to daily level\n",
    "ts = ts.groupby(by=['x', 'cat'], as_index=False)['y'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill dates\n",
    "\n",
    "For each category, fill dates between min(category's GR date) until `max_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from nvtsbe_planning.common.pogr import fill_dt_all\n",
    "ts_filled = fill_dt_all(ts, ts_id=['cat'], dates=(\"min\", max_date, \"D\"), freq=freq)\n",
    "\n",
    "# Make sure each timeseries starts at their original earliest date, instead of 2017-01-01.\n",
    "ts_filled.groupby(by='cat', as_index=False)['x'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate gluonts TRAIN dataset\n",
    "\n",
    "Implementation notes:\n",
    "\n",
    "- `df2glounts()` is done in the spirit of bias-for-action. However, in the spirit of invent-and-simplify and insist-on-highest-standard, we should fix port `csv2deepar` submodule from 1P to glounts.\n",
    "\n",
    "- We keep all gluonts timeseries in memory. This is not memory-optimal, however it's simple to implement.\n",
    "\n",
    "- We use `gluonts.dataset.common.save_datasets()` but note that this function writes only to local filesystem, hence needs a follow-up step to upload to s3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN: in-memory gluonts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_cat(cats):\n",
    "    # FIXME: pulled to nvtsbe_planning.common\n",
    "    return {c:i for i,c in enumerate(cats)}\n",
    "\n",
    "def df2gluonts(df, cat_idx, fcast_len=12, freq='W', ts_id=['cat', 'cc'], static_cat=['cat', 'cc'], item_id_fn=None):\n",
    "    # FIXME:\n",
    "    # - This hack is bias-for-action.\n",
    "    # - For invent-and-simplify + insist-on-highest-standard: fix module csv2deepar, which checks for missing ts, etc.\n",
    "    data_iter = []\n",
    "\n",
    "    # Build the payload\n",
    "    for item_id, dfg in df.groupby(ts_id, as_index=False):\n",
    "        if len(ts_id) < 2:\n",
    "            item_id = [item_id]\n",
    "\n",
    "        if fcast_len > 0:  # Typically for training\n",
    "            ts_len = len(dfg) - fcast_len\n",
    "            target = dfg['y'][:-fcast_len]\n",
    "        else:\n",
    "            target = dfg['y']\n",
    "\n",
    "        feat_static_cat = []\n",
    "        for col in static_cat:\n",
    "            assert dfg[col].nunique() == 1\n",
    "            cat_value = dfg[col].iloc[0]\n",
    "            feat_static_cat.append(cat_idx[col][cat_value])  # Encoded cat values to appear in feat_static_cat.\n",
    "\n",
    "        if item_id_fn is None:\n",
    "            item_id = '|'.join(item_id)  # NOTE: sm-glounts entrypoint will plot '|' as '\\n'.\n",
    "        else:\n",
    "            item_id = item_id_fn(*item_id)\n",
    "\n",
    "        data_iter.append({\n",
    "            'start': dfg.iloc[0]['x'],\n",
    "            'target': target,\n",
    "            'feat_static_cat': feat_static_cat,\n",
    "            'item_id': item_id\n",
    "        })\n",
    "\n",
    "    data = ListDataset(data_iter, freq = freq)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_inverted_idx = {'cat': encode_cat(ts_filled['cat'].unique())}\n",
    "\n",
    "# Drop the final fcast_length from train data.\n",
    "train_data= df2gluonts(ts_filled, cat_inverted_idx, fcast_len=fcast_length, freq=freq, ts_id=['cat'], static_cat=['cat'])\n",
    "\n",
    "# Test data include fcast_length which are ground truths.\n",
    "test_data = df2gluonts(ts_filled, cat_inverted_idx, fcast_len=0, freq=freq, ts_id=['cat'], static_cat=['cat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track statistics\n",
    "(stats\n",
    "     + ['train_ts_count', len(train_data.list_data)]\n",
    "     + ['test_ts_count', len(test_data.list_data)]\n",
    ")\n",
    "\n",
    "stats.df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN: write to local fs, then s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gluonts_datasets = TrainDatasets(\n",
    "    metadata=MetaData(\n",
    "                freq=freq,\n",
    "                target={'name': 'gr'},\n",
    "                feat_static_cat=[\n",
    "                    CategoricalFeatureInfo(name=k, cardinality=len(v)+1)   # Add 'unkown'.\n",
    "                    for k,v in cat_inverted_idx.items()\n",
    "                ],\n",
    "                prediction_length = fcast_length\n",
    "    ),\n",
    "    train=train_data,\n",
    "    test=test_data\n",
    ")\n",
    "\n",
    "# Setting `overwrite=True` means rm -fr path_str, mkdir path_str, then write individual files.\n",
    "local_path=f'../../data/processed/{dataset_name}'\n",
    "save_datasets(\n",
    "    dataset=gluonts_datasets,\n",
    "    path_str=local_path,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Save also our indexes\n",
    "with open(Path(local_path) / 'metadata' / 'cat.json', 'w') as f:\n",
    "    json.dump(cat_inverted_idx, f)\n",
    "    \n",
    "%set_env LOCAL_PATH=$local_path\n",
    "!cat $LOCAL_PATH/metadata/metadata.json | head -1 | jq\n",
    "!cat $LOCAL_PATH/train/data.json | head -1 | jq\n",
    "!cat $LOCAL_PATH/test/data.json | head -1 | jq\n",
    "!cat $LOCAL_PATH/metadata/cat.json | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can re-read the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "reloaded_dataset = load_datasets(\n",
    "                        metadata=os.path.join(local_path, \"metadata\"),\n",
    "                        train=os.path.join(local_path, \"train\"),\n",
    "                        test=os.path.join(local_path, \"test\")\n",
    "                   )\n",
    "display(\n",
    "    reloaded_dataset.metadata,\n",
    "    reloaded_dataset.train,\n",
    "    reloaded_dataset.test\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks ok. Now, let's upload to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $LOCAL_PATH s3://$BUCKET/$PREFIX/$DATASET_NAME/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Recap on stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
