{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<div style='font-size:200%'>Batch Transform using the gluonts entrypoint</div>**\n",
    "\n",
    "In this notebook, we first register a model artifact into a SageMaker model, then perform a batch evaluation. Optionally, we deregister the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import logging\n",
    "import sagemaker as sm\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "\n",
    "from smallmatter.sm import get_sm_execution_role, get_model_and_output_tgz\n",
    "\n",
    "# smallmatter.sm.get_sm_execution_role() will:\n",
    "# - on SageMaker classic notebook instance, simply call sagemaker.get_execution_role()\n",
    "# - outside of SageMaker classic notebook instance, return the first role whose name\n",
    "#   startswith \"AmazonSageMaker-ExecutionRole-\"\n",
    "role: str = get_sm_execution_role()\n",
    "\n",
    "sess = sm.Session()\n",
    "region: str = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'BUCKETNAME'\n",
    "\n",
    "# I/O S3 paths MUST have trailing '/'\n",
    "bt_input = f's3://{bucket}/gluonts-examples-dataset/synthetic-dataset/test/'   # Reuse test-split from notebook 01.\n",
    "bt_output = f's3://{bucket}/bt_output/'\n",
    "\n",
    "# Use artifacts from this training job.\n",
    "train_job = \"mxnet-training-2020-10-21-18-56-10-341\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observe training results\n",
    "\n",
    "As in any SageMaker training job, entrypoint script will generate two artifacts in the S3: `model.tar.gz` and `output.tar.gz`.\n",
    "\n",
    "The `model.tar.gz` contains the persisted model that can be used later on for inference.\n",
    "\n",
    "The `output.tar.gz` contains the following:\n",
    "- individual plot of each test timeseries\n",
    "- montage of plots of all test timeseries\n",
    "- backtest evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tgz, output_tgz = (str(path) for path in get_model_and_output_tgz(train_job))\n",
    "\n",
    "%set_env MODEL_S3=$model_tgz\n",
    "%set_env OUTPUT_S3=$output_tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo -e \"\\nModel artifacts $MODEL_S3:\"\n",
    "aws s3 cp $MODEL_S3 - | tar -tzvf -\n",
    "\n",
    "echo -e \"\\nOutput $OUTPUT_S3:\"\n",
    "aws s3 cp $OUTPUT_S3 - | tar -tzvf - | head  # NOTE: \"[Errno 32] Broken pipe\" can be safely ignored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model\n",
    "\n",
    "Let SDK auto-generates the model name, so we can safely make this notebook reentrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_model = MXNetModel(\n",
    "        model_data=model_tgz,\n",
    "        role=role,\n",
    "        entry_point='inference.py',\n",
    "        source_dir='../src/entrypoint',\n",
    "        py_version=\"py3\",\n",
    "        framework_version=\"1.6.0\",\n",
    "        sagemaker_session=sess,\n",
    "        container_log_level=logging.DEBUG,   # Comment this line to reduce the amount of logs in CloudWatch.\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bit of reverse engineering, to confirm env. vars that the model will end-up using. Will be useful when the time comes where I need to do all these in boto3 or botocore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before create model\n",
    "mxnet_model._framework_env_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "mxnet_model._create_sagemaker_model(instance_type='ml.m5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model name\n",
    "mxnet_model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_model._framework_env_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peek into model's model.tar.gz (which is different from training artifact model.tar.gz).\n",
    "model_s3 = mxnet_model._framework_env_vars()['SAGEMAKER_SUBMIT_DIRECTORY']\n",
    "%set_env MODEL_S3=$model_s3\n",
    "!aws s3 cp $MODEL_S3 - | tar -tzvf -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Transform\n",
    "bt = mxnet_model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    strategy='MultiRecord',\n",
    "    assemble_with='Line',\n",
    "    output_path=bt_output,\n",
    "    accept='application/json',\n",
    "    env={'SAGEMAKER_MODEL_SERVER_TIMEOUT': '3600'},\n",
    "    max_payload=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt.base_transform_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting wait=False (which is the default) frees this notebook\n",
    "# from getting blocked by the transform job.\n",
    "bt.transform(\n",
    "    data=bt_input,\n",
    "    data_type='S3Prefix',\n",
    "    content_type='application/json',\n",
    "    split_type='Line',\n",
    "    wait=False,\n",
    "    logs=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting `wait=False` (which is the default for transform jobs), while the transform job is running, you can may shutdown this notebook's kernel, close this notebook, and go to the SageMaker console to monitor the batch-transform progress. The batch-transform job's console also contains links to CloudWatch log.\n",
    "\n",
    "Once the job finishes, from the batch-transform job's console, you can follow through the S3 output location, where you can preview or download the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete model\n",
    "\n",
    "Uncomment and execute cell to \"deregister\" the model from SageMaker. The inference model artifacts remain untouched in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mxnet_model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
